{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelgtrejo/Evaluacion-by-Angel/blob/main/Copia_de_FaceRecognition_JetBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reconocimiento Facial con FaceNet — Preparación para JetBot (ROS Melodic)\n",
        "\n",
        "Este cuaderno te guiará paso a paso para:\n",
        "\n",
        "1. **Entrenar un modelo** de reconocimiento facial usando *FaceNet* (para obtener \"embeddings\" faciales).\n",
        "2. **Entrenar un clasificador SVM** para distinguir rostros.\n",
        "3. **Guardar el modelo** en formatos portables (`.pkl` y `.onnx`) que se ejecutarán después en tu JetBot Nano con Python 2.7.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Configuración del entorno\n",
        "\n",
        "Asegúrate de tener la GPU activada:  \n",
        "> Menú → Entorno de ejecución → Cambiar tipo de entorno → Hardware acelerador → GPU"
      ],
      "metadata": {
        "id": "VZb_3frt4aba"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H048n4sO4Wn_"
      },
      "outputs": [],
      "source": [
        "!pip install facenet-pytorch torch torchvision scikit-learn opencv-python matplotlib joblib onnx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Importación de librerías"
      ],
      "metadata": {
        "id": "VCLiftda47Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib"
      ],
      "metadata": {
        "id": "UMNylY3N46nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Carga de imágenes de entrenamiento\n",
        "\n",
        "Para este ejemplo, crearemos una pequeña base de datos:\n",
        "- Cada subcarpeta dentro de `data/` contendrá imágenes de una persona.\n",
        "\n",
        "Puedes subir tus propias imágenes (por ejemplo, de tus compañeros) o usar imágenes descargadas de Internet.\n"
      ],
      "metadata": {
        "id": "CSCn2Q_a5DGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "!mkdir -p data\n",
        "print(\"Crea una carpeta data/persona1, data/persona2, etc. y sube tus fotos\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "for name, file in uploaded.items():\n",
        "    with open(os.path.join(\"data\", name), \"wb\") as f:\n",
        "        f.write(file)\n",
        "print(\"Archivos cargados.\")\n"
      ],
      "metadata": {
        "id": "ygk_9UlO5I9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Extracción de rostros y embeddings\n",
        "Usaremos **MTCNN** para detectar rostros y **FaceNet (InceptionResnetV1)** para convertirlos en vectores numéricos (*embeddings*).\n"
      ],
      "metadata": {
        "id": "5F5bfYSt5T9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "mtcnn = MTCNN(image_size=160, margin=20, device=device)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "for person in os.listdir(\"data\"):\n",
        "    person_path = os.path.join(\"data\", person)\n",
        "    if not os.path.isdir(person_path):\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(person_path):\n",
        "        path = os.path.join(person_path, file)\n",
        "        img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "        face = mtcnn(img)\n",
        "        if face is not None:\n",
        "            emb = resnet(face.unsqueeze(0).to(device)).detach().cpu().numpy()\n",
        "            X.append(emb.flatten())\n",
        "            y.append(person)\n",
        "\n",
        "print(\"Rostros procesados:\", len(X))\n"
      ],
      "metadata": {
        "id": "qvp75wuT5Yp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Entrenamiento del clasificador SVM\n",
        "\n",
        "El clasificador SVM aprenderá a diferenciar las personas según los *embeddings* de sus rostros.\n"
      ],
      "metadata": {
        "id": "nDTPiwva5edZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "y_enc = encoder.fit_transform(y)\n",
        "\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(X, y_enc)\n",
        "\n",
        "print(\"Clasificador entrenado con\", len(set(y_enc)), \"personas.\")\n"
      ],
      "metadata": {
        "id": "xPFWm06Q5iUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Prueba rápida de predicción\n",
        "Sube una foto de prueba y verifica qué persona detecta.\n"
      ],
      "metadata": {
        "id": "Z0o82q0r5ls8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "test_img_path = list(uploaded.keys())[0]\n",
        "\n",
        "img = cv2.cvtColor(cv2.imread(test_img_path), cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.title(\"Imagen de prueba\")\n",
        "plt.show()\n",
        "\n",
        "face = mtcnn(img)\n",
        "if face is not None:\n",
        "    emb = resnet(face.unsqueeze(0).to(device)).detach().cpu().numpy()\n",
        "    pred = model.predict(emb)\n",
        "    name = encoder.inverse_transform(pred)[0]\n",
        "    print(\"Predicción:\", name)\n",
        "else:\n",
        "    print(\"No se detectó ningún rostro.\")\n"
      ],
      "metadata": {
        "id": "B_2l4WSG5pH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Guardar los modelos para la Jetson Nano\n",
        "Guardaremos:\n",
        "- El clasificador y el codificador (`.pkl`)\n",
        "- El modelo FaceNet en formato **ONNX** (para poder usarlo desde Python 2.7 con `onnxruntime`)\n"
      ],
      "metadata": {
        "id": "Ljq3VDOu5tuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model, 'face_svm.pkl')\n",
        "joblib.dump(encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Exportar FaceNet a ONNX\n",
        "dummy = torch.randn(1, 3, 160, 160).to(device)\n",
        "torch.onnx.export(resnet, dummy, \"facenet.onnx\", opset_version=11)\n",
        "print(\"Archivos guardados: face_svm.pkl, label_encoder.pkl, facenet.onnx\")\n"
      ],
      "metadata": {
        "id": "sHn8R9H85xap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Descargar modelos entrenados\n",
        "\n"
      ],
      "metadata": {
        "id": "csd0NSpq50lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('face_svm.pkl')\n",
        "files.download('label_encoder.pkl')\n",
        "files.download('facenet.onnx')"
      ],
      "metadata": {
        "id": "_Ndgqf_d53WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Qué hacer después\n",
        "\n",
        "1. **Copia los archivos al JetBot Nano (Python 2.7):**\n",
        "   ```bash\n",
        "   scp face_svm.pkl label_encoder.pkl facenet.onnx jetbot@<IP_JETBOT>:/home/jetbot/models/\n",
        "   ```\n",
        "2. **En la Jetson**, usa el script ROS 1 (Python 2.7) para ejecutar la inferencia facial:\n",
        "- Lee las imágenes de la cámara.\n",
        "- Carga `facenet.onnx` con onnxruntime.\n",
        "- Clasifica con `face_svm.pkl`.\n",
        "- Publica el nombre en `/face_recognition/name`.\n",
        "\n",
        "3. **Desde otro nodo ROS**, controla el movimiento según el rostro reconocido."
      ],
      "metadata": {
        "id": "w3acpxmq56hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflexión\n",
        "\n",
        "- En Colab usamos *Python 3 y GPU* → ideal para entrenamiento.\n",
        "- En JetBot usamos *Python 2.7 y CPU/GPU embebida* → ideal para inferencia."
      ],
      "metadata": {
        "id": "hiCThVvn5X_y"
      }
    }
  ]
}